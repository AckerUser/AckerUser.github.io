# Python爬虫知识点

> 爬虫流程大致分为：数据的爬取、数据的清洗、数据的存储

## 一、数据的爬取

> 类库：`urllib.request`、`urllib.request.Request`

### urllib.request

#### 	1. 简介

urlopen()方法可以实现最基本的请求的发起
使用方法为：

`urllib.request.urlopen （url，data = None，[ timeout*，] *，cafile = None，capath = None，cadefault = False，context = None ` 

#### 	2. 参数

| 参数    | 作用                                                         |
| ------- | ------------------------------------------------------------ |
| url     | 要请求的url                                                  |
| data    | data必须是bytes(字节流）类型，如果是字典，可以用urllib.parse模块里的urlencode()编码 |
| timeout | 以秒为单位指定用于阻止诸如连接尝试之类的操作的超时           |
| cafile  | *cafile*应该指向包含*一堆*CA证书的单个文件                   |
| capath  | *capath*应该指向哈希证书文件的目录                           |
| context | 返回一个可以用作[上下文管理器](https://docs.python.org/3/glossary.html#term-context-manager)的对象，该对象 具有*url*，*headers*和*status*属性 |

#### 	3. 用法

```python
from urllib import request, parse

# GET写法
#res = request.urlopen('https://httpbin.org/get')

# POST写法
# 封装参数
dict = {
    'username': '123456',
    'password': '123456'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
res = request.urlopen('https://httpbin.org/post',data=data)
# 指定的编码格式解码字符串
print(res.read().decode('utf-8'))
```

### urllib.request.Request

#### 	1. 简介

urlopen()方法可以实现最基本的请求的发起，但如果要加入Headers等信息，就可以利用Request类来构造请求。
使用方法为：

`urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)`

#### 	2. 参数

| 参数            | 作用                                                         |
| --------------- | ------------------------------------------------------------ |
| url             | 要请求的url                                                  |
| data            | data必须是bytes(字节流）类型，如果是字典，可以用urllib.parse模块里的urlencode()编码 |
| headers         | headers是一个字典类型，是请求头。可以在构造请求时通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加。可以通过请求头伪装浏览器，默认User-Agent是Python-urllib。要伪装火狐浏览器，可以设置`User-Agent为Mozilla/5.0 (x11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11` |
| origin_req_host | 指定请求方的host名称或者ip地址                               |
| unverifiable    | 设置网页是否需要验证，默认是False，这个参数一般也不用设置。  |
| method          | method是一个字符串，用来指定请求使用的方法，比如GET，POST和PUT等。 |

#### 	3. 用法

```python
from urllib import request, parse

# GET请求
# req = request.Request('https://httpbin.org/get')
# res = request.urlopen(req)
# print(res.read().decode('utf-8'))

# POST请求
url = 'http://httpbin.org/post'
# 封装请求头
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
# 封装参数
datas = {
    'username': '123456',
    'password': '123456'
}
# 参数编码转换+二进制
data = bytes(parse.urlencode(datas), encoding='utf8')
# 第一种写法
req = request.Request(url=url, data=data, headers=headers, method='POST')
# 第二种写法
#req = request.Request(url=url, data=data, method='POST')
#req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')
res = request.urlopen(req)
# 指定的编码格式解码字符串
print(res.read().decode('utf-8'))
```

#### 

## 二、数据的清洗

> 类库：`BeautifulSoup`、`re`

### BeautifulSoup

#### 	1. 简介

* **Beautiful Soup** 是一个可以从 **HTML** 或 **XML** 文件中提取数据的 **Python** 库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式
* 不仅支持 **HTML** 解析器,还支持一些第三方的解析器，如，**lxml，XML，html5lib** 但是需要安装相应的库 lxml、html5lib
* 要解析的文档是什么类型: 目前支持, **html, xml,** 和 **html5**
* 指定使用哪种解析器: 目前支持, **lxml, html5lib,** 和 **html.parser**

#### 	2.分类

* `Tag`：通俗点讲就是 **HTML** 中的一个个标签，像上面的 `div，p`。每个 `Tag` 有两个重要的属性 `name` 和 `attrs，name` 指标签的名字或者 `tag` 本身的 `name，attrs` 通常指一个标签的 `class`。
* `NavigableString`：获取标签内部的文字，如，`soup.p.string`。
* `BeautifulSoup`：表示一个文档的全部内容。
* `Comment：Comment` 对象是一个特殊类型的 `NavigableString` 对象，其输出的内容不包括注释符号

* #### find_all( name , attrs , recursive , string , **kwargs )

按 `name` 搜索: `name` 参数可以查找所有名字为 `name` 的 `tag`,字符串对象会被自动忽略掉:

```bash
 soup.find_all("li")
```

按 `id` 搜索: 如果包含一个名字为 `id` 的参数,搜索时会把该参数当作指定名字 `tag` 的属性来搜索:

```bash
 soup.find_all(id='link2')
```

按 `attr` 搜索：有些 `tag` 属性在搜索不能使用,比如 **HTML5** 中的 `data-*` 属性，但是可以通过 `find_all()` 方法的 `attrs` 参数定义一个字典参数来搜索包含特殊属性的 `tag`:

```bash
 data_soup.find_all(attrs={"data-foo": "value"})
```

按 `CSS` 搜索: 按照 `CSS` 类名搜索 `tag` 的功能非常实用,但标识`CSS` 类名的关键字 `class` 在 **Python** 中是保留字,使用 `class` 做参数会导致语法错误.从 **Beautiful Soup** 的 4.1.1 版本开始,可以通过 `class_` 参数搜索有指定 `CSS` 类名的 `tag`:

```ruby
 soup.find_all('li', class_="have-img")
```

`string` 参数：通过 `string` 参数可以搜搜文档中的字符串内容.与 `name` 参数的可选值一样, `string` 参数接受 字符串 , 正则表达式 , 列表, `True` 。 看例子:

```cpp
 soup.find_all("a", string="Elsie")
```

`recursive` 参数：调用 `tag` 的 `find_all()` 方法时,**Beautiful Soup** 会检索当前 `tag` 的所有子孙节点,如果只想搜索 `tag` 的直接子节点,可以使用参数 `recursive=False` .

```php
 soup.find_all("title", recursive=False)
```

* #### select()

* 我们在写 CSS 时，标签名不加任何修饰，类名前加点，id名前加 #，在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list
  **（1）通过标签名查找**

  ```
  print soup.select('title') 
  print soup.select('a')
  ```

  **（2）通过id名查找** 

  ```
  print soup.select('#lin1')
  ```

  **（3）通过 类名查找** 

  ```
  print soup.select('.link1')
  ```

  **（4）组合查找**

  组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开

  ```
  print soup.select('p #link1')
  #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]
  ```

  直接子标签查找

  ```
  print soup.select("head > title")
  #[<title>The Dormouse's story</title>]
  ```

  **（5）属性查找**

  查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。

  ```
  print soup.select("head > title")
  #[<title>The Dormouse's story</title>]
   
  print soup.select('a[href="http://example.com/elsie"]')
  #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]
  ```

  同样，属性仍然可以与上述查找方式组合，不在同一节点的空格隔开，同一节点的不加空格

  ```
  print soup.select('p a[href="http://example.com/elsie"]')
  #[<a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>]
  ```

####  3.用法

```python
from bs4 import BeautifulSoup

# 获取输入流
f = open("./demo01.html","rb")
# 获取bs对象
bs = BeautifulSoup(f,'html.parser')
# 查询所有a标签
bsa = bs.div.find_all("a")
print("find_all('a')",len(bsa))
print("select(.'sister1')",bs.select(".sister1"))
print("text='aaa'",bs.div.find_all(text="aaa))
......
```

## 三、数据的存储

> 类库：`xlwt`、`sqlite3`

### xlwt

#### 	1. 简介

 xlwt是一个库，用于将数据和格式化信息写入旧的Excel文件(即:.xls)**。注意，这里是写入老版本的Excel，新版本后缀.xlsx目前还不支持** 

#### 	2. 用法

```python
# 0.导入xlwt
import  xlwt

# 1.创建workbook对象
workbook =xlwt.Workbook(encoding ="utf-8",style_compression=0)

# 2.创建一个sheet对象,一个sheet对象对应excel文件中一张表格.
# Cell_overwirte_ok 是能够覆盖单元表格的意思
sheet =workbook.add_sheet("2",cell_overwrite_ok=True)

# 3.向表中添加数据.
for i in range(9):
    for j in range(i+1):
        sheet.write(i,j,"%d*%d=%d"%(i+1,j+1,(i+1)*(j+1)))
        # print(i,j)
# #4.保存.
workbook.save(r"test.xls")
```

### sqlite3

#### 	1. 简介

SQLite是一个进程内的库，实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。它是一个零配置的数据库，这意味着与其他数据库不一样，您不需要在系统中配置。就像其他数据库，SQLite 引擎不是一个独立的进程，可以按应用程序需求进行静态或动态连接。SQLite 直接访问其存储文件。

* 优势
  * 不需要一个单独的服务器进程或操作的系统（无服务器的）。
  * SQLite 不需要配置，这意味着不需要安装或管理。
  * 一个完整的 SQLite 数据库是存储在一个单一的跨平台的磁盘文件。
  * SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于250KiB。
  * SQLite 是自给自足的，这意味着不需要任何外部的依赖。
  * SQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。
  * SQLite 支持 SQL92（SQL2）标准的大多数查询语言的功能。
  * SQLite 使用 ANSI-C 编写的，并提供了简单和易于使用的 API。
  * SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。

#### 2. 用法

```python
#!/usr/bin/env python3
# encoding: utf-8

"""
@version: 3.8.5
@author: acker
@file: demo_sqlite3.py
@date: 2020/10/27 9:27
"""
import sqlite3

conn = sqlite3.connect("data.db")
cur = conn.cursor()
# 创建表结构
# sql = '''
#       create table test(
#       id INTEGER primary key autoincrement,
#       name varchar
#       )
# 添加数据
# '''
# sql = '''
#       insert into test values (null,'校长')
# '''
# 删除数据
# sql = ''''
#       delete from test
# '''
# 修改数据
# sql = '''
#       update test set name = "小明"
# '''
# 查询数据
sql = '''
      select * from test
'''
result = cur.execute(sql)
for item in result:
    print(item)
conn.commit()
conn.close()
print("操作完成")
```

##### 